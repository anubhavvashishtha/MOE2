{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SCRIPT 1: LOCAL LORA MERGER\n",
    "Run this on your local computer (CPU is fine)\n",
    "This merges your LoRA weights with the base model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SCRIPT 1: MERGING LORA WEIGHTS LOCALLY\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - CHANGE THESE PATHS\n",
    "# ============================================\n",
    "FINETUNED_MODEL_PATH = \"./tinyllama-anxity-chat\"  # Your fine-tuned model folder\n",
    "CHECKPOINT = \"checkpoint-1155\"                # Your checkpoint folder name\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"merged_model_for_mobile\"\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: MERGE LORA WEIGHTS\n",
    "# ============================================\n",
    "\n",
    "checkpoint_path = os.path.join(FINETUNED_MODEL_PATH, CHECKPOINT)\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   Fine-tuned model: {FINETUNED_MODEL_PATH}\")\n",
    "print(f\"   Checkpoint: {CHECKPOINT}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print()\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(FINETUNED_MODEL_PATH):\n",
    "    print(f\"‚ùå Error: Model path not found: {FINETUNED_MODEL_PATH}\")\n",
    "    print(\"   Please update FINETUNED_MODEL_PATH in the script\")\n",
    "    exit(1)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"‚ùå Error: Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"   Available checkpoints in {FINETUNED_MODEL_PATH}:\")\n",
    "    for item in os.listdir(FINETUNED_MODEL_PATH):\n",
    "        if item.startswith(\"checkpoint-\"):\n",
    "            print(f\"   - {item}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"‚úÖ Paths validated\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 1/4: Loading Base Model\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüì• Loading base model: {BASE_MODEL}\")\n",
    "    print(\"   (This may take 2-5 minutes...)\\n\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Base model loaded successfully!\")\n",
    "    print(f\"   Model size: {sum(p.numel() for p in base_model.parameters()) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 2/4: Loading Your LoRA Weights\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüì• Loading LoRA adapters from: {checkpoint_path}\\n\")\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    \n",
    "    print(\"‚úÖ LoRA weights loaded successfully!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3/4: Merging LoRA into Base Model\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüîÑ Merging weights...\")\n",
    "    print(\"   (This may take 5-10 minutes...)\\n\")\n",
    "    \n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    print(\"‚úÖ Models merged successfully!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 4/4: Saving Merged Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Saving merged model to: {OUTPUT_DIR}\")\n",
    "    print(\"   (This may take 5-10 minutes...)\\n\")\n",
    "    \n",
    "    # Save model\n",
    "    merged_model.save_pretrained(OUTPUT_DIR, max_shard_size=\"2GB\")\n",
    "    \n",
    "    # Load and save tokenizer\n",
    "    print(\"üì• Loading and saving tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(\"‚úÖ Tokenizer saved!\")\n",
    "    \n",
    "    # Save metadata for Script 2\n",
    "    metadata = {\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"original_checkpoint\": checkpoint_path,\n",
    "        \"merged_date\": str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"),\n",
    "        \"model_type\": \"TinyLlama-1.1B\",\n",
    "        \"chat_template\": tokenizer.chat_template if hasattr(tokenizer, 'chat_template') else None,\n",
    "        \"bos_token\": tokenizer.bos_token,\n",
    "        \"eos_token\": tokenizer.eos_token,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, \"conversion_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Metadata saved!\")\n",
    "    \n",
    "    # ============================================\n",
    "    # SUCCESS - SHOW NEXT STEPS\n",
    "    # ============================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ SUCCESS! MERGED MODEL READY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Your merged model is saved in: {OUTPUT_DIR}/\")\n",
    "    print(\"\\nüìÅ Files created:\")\n",
    "    for file in os.listdir(OUTPUT_DIR):\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   - {file} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    total_size = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR))\n",
    "    print(f\"\\nüìä Total size: {total_size / (1024 * 1024):.1f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üì± NEXT STEPS:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "1. üì§ Upload the '{OUTPUT_DIR}' folder to Google Drive\n",
    "   (You can zip it first to make upload faster)\n",
    "\n",
    "2. üöÄ Open Google Colab: https://colab.research.google.com\n",
    "\n",
    "3. üìã Copy and run SCRIPT 2 (the GPU conversion script)\n",
    "\n",
    "4. üì• Download the .litertlm file from Colab\n",
    "\n",
    "5. üì± Transfer to your phone and import via the '+' button!\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ Script 1 Complete! Ready for Script 2 on Colab\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   - Make sure you have enough RAM (at least 8GB free)\")\n",
    "    print(\"   - Check that all required packages are installed:\")\n",
    "    print(\"     pip install transformers peft torch\")\n",
    "    print(\"   - Verify your checkpoint path is correct\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb18fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ai-edge-torch\n",
    "%pip install ai-edge-torch-generative\n",
    "%pip install torch transformers\n",
    "%pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b17ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
